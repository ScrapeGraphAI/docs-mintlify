---
title: 'SmartScraper'
description: 'AI-powered web scraping for any website'
icon: 'robot'
---

<Frame>
  <img src="/services/images/smartscraper-banner.png" alt="SmartScraper Service" />
</Frame>

## Overview

SmartScraper is our flagship LLM-powered web scraping service that intelligently extracts structured data from any website. Using advanced LLM models, it understands context and content like a human would, making web data extraction more reliable and efficient than ever.

<Note>
Try SmartScraper instantly in our [interactive playground](https://dashboard.scrapegraphai.com/) - no coding required!
</Note>

## Getting Started

### Quick Start

<CodeGroup>

```python Python
from scrapegraph_py import Client

client = Client(api_key="your-api-key")

response = client.smartscraper(
    website_url="https://scrapegraphai.com/",
    user_prompt="Extract info about the company"
)
```

```javascript JavaScript
import { Client } from 'scrapegraph-js';

// Initialize the client
const sgai_client = new Client("your-api-key");

try {
  const response = await sgai_client.smartscraper({
    websiteUrl: "https://example-shop.com/products/456",
    userPrompt: "Extract product details including name, price, availability, specifications, and customer ratings."
  });

  console.log('Request ID:', response.requestId);
  console.log('Result:', response.result);
} catch (error) {
  console.error(error);
} finally {
  sgai_client.close();
}
```

```bash cURL
curl -X 'POST' \
  'https://api.scrapegraphai.com/v1/smartscraper' \
  -H 'accept: application/json' \
  -H 'SGAI-APIKEY: your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
  "website_url": "https://example-shop.com/products/456",
  "user_prompt": "Extract product details including name, price, availability, specifications, and customer ratings."
}'
```

</CodeGroup>

<Note>
Get your API key from the [dashboard](https://dashboard.scrapegraphai.com)
</Note>


<Accordion title="Example Response" icon="terminal">
```json
{
  "request_id": "sg-req-abc123",
  "status": "completed",
  "website_url": "https://scrapegraphai.com/",
  "user_prompt": "Extract info about the company",
  "result": {
    "company_name": "ScrapeGraphAI",
    "description": "ScrapeGraphAI is a powerful AI scraping API designed for efficient web data extraction to power LLM applications and AI agents...",
    "features": [
      "Effortless, cost-effective, and AI-powered data extraction",
      "Handles proxy rotation and rate limits",
      "Supports a wide variety of websites"
    ],
    "contact_email": "contact@scrapegraphai.com",
    "social_links": {
      "github": "https://github.com/ScrapeGraphAI/Scrapegraph-ai",
      "linkedin": "https://www.linkedin.com/company/101881123",
      "twitter": "https://x.com/scrapegraphai"
    }
  },
  "error": ""
}
```

The response includes:
- `request_id`: Unique identifier for tracking your request
- `status`: Current status of the extraction ("completed", "running", "failed")
- `result`: The extracted data in structured JSON format
- `error`: Error message (if any occurred during extraction)
</Accordion>

<Accordion title="Using Your Own HTML" icon="code">
Instead of providing a URL, you can optionally pass your own HTML content:

```python
html_content = """
<html>
    <body>
        <h1>ScrapeGraphAI</h1>
        <div class="description">
            <p>AI-powered web scraping for modern applications.</p>
        </div>
        <div class="features">
            <ul>
                <li>Smart Extraction</li>
                <li>Local Processing</li>
                <li>Schema Support</li>
            </ul>
        </div>
    </body>
</html>
"""

response = client.smartscraper(
    website_html=html_content,  # This will override website_url if both are provided
    user_prompt="Extract info about the company"
)
```

This is useful when:
- You already have the HTML content cached
- You want to process modified HTML
- You're working with dynamically generated content
- You need to process content offline
- You want to pre-process the HTML before extraction

<Note>
When both `website_url` and `website_html` are provided, `website_html` takes precedence and will be used for extraction.
</Note>
</Accordion>

## Key Features

<CardGroup cols={2}>
  <Card title="Universal Compatibility" icon="globe">
    Works with any website structure, including JavaScript-rendered content
  </Card>
  <Card title="AI Understanding" icon="brain">
    Contextual understanding of content for accurate extraction
  </Card>
  <Card title="Structured Output" icon="table">
    Returns clean, structured data in your preferred format
  </Card>
  <Card title="Schema Support" icon="code">
    Define custom output schemas using Pydantic or Zod
  </Card>
</CardGroup>

## Use Cases

### Content Aggregation
- News article extraction
- Blog post summarization
- Product information gathering
- Research data collection

### Data Analysis
- Market research
- Competitor analysis
- Price monitoring
- Trend tracking

### AI Training
- Dataset creation
- Training data collection
- Content classification
- Knowledge base building

<Note>
Want to learn more about our AI-powered scraping technology? Visit our [main website](https://scrapegraphai.com) to discover how we're revolutionizing web data extraction.
</Note>

## Advanced Usage

### Request Helper Function

The `getSmartScraperRequest` function helps create properly formatted request objects for the SmartScraper service:

```javascript
import { getSmartScraperRequest } from 'scrapegraph-js';

const request = getSmartScraperRequest({
  websiteUrl: "https://example.com",
  userPrompt: "Extract product details including name, price, and description",
  headers: {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  },
  outputSchema: {
    name: { type: "string" },
    price: { type: "number" },
    description: { type: "string" }
  }
});
```

#### Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| websiteUrl | string | Yes* | The URL of the website to scrape. Either websiteUrl or websiteHtml must be provided. |
| userPrompt | string | Yes | Natural language description of what data to extract. |
| websiteHtml | string | No | HTML content to process instead of fetching from URL. |
| headers | object | No | Custom headers for the request (e.g., User-Agent, cookies). |
| outputSchema | object | No | Schema defining the structure of the extracted data. |

\* Either websiteUrl or websiteHtml must be provided.

#### Return Value

Returns an object with the following structure:

```typescript
{
  request_id: string;
  status: "queued" | "processing" | "completed" | "failed";
  website_url?: string;
  user_prompt: string;
  result?: object | null;
  error: string;
}
```

#### Error Handling

The function includes robust error handling for various scenarios:

```javascript
try {
  const request = getSmartScraperRequest({
    websiteUrl: "https://example.com/products",
    userPrompt: "Extract all product information"
  });
} catch (error) {
  switch (error.code) {
    case 'INVALID_URL':
      console.error('The provided URL is not valid');
      break;
    case 'INVALID_HTML':
      console.error('The provided HTML content is invalid');
      break;
    case 'SCHEMA_VALIDATION':
      console.error('Output schema validation failed:', error.details);
      break;
    case 'MISSING_REQUIRED':
      console.error('Required parameters are missing');
      break;
    default:
      console.error('An unexpected error occurred:', error);
  }
}
```

#### Advanced Examples

##### E-commerce Product Extraction

```javascript
const request = getSmartScraperRequest({
  websiteUrl: "https://example.com/products/category",
  userPrompt: "Extract all products from this category page",
  headers: {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Cookie": "currency=USD; country=US"
  },
  outputSchema: {
    category: { type: "string" },
    products: {
      type: "array",
      items: {
        type: "object",
        properties: {
          id: { type: "string" },
          name: { type: "string" },
          price: { type: "number" },
          currency: { type: "string" },
          availability: { type: "boolean" },
          variants: {
            type: "array",
            items: {
              type: "object",
              properties: {
                size: { type: "string" },
                color: { type: "string" },
                stock: { type: "number" }
              }
            }
          }
        }
      }
    }
  }
});
```

##### Processing Dynamic Content

```javascript
// First, get the dynamic content
const dynamicHtml = await fetchDynamicContent("https://example.com/spa");

const request = getSmartScraperRequest({
  websiteHtml: dynamicHtml,
  userPrompt: "Extract the main content section",
  headers: {
    "X-Requested-With": "XMLHttpRequest",
    "Accept": "application/json"
  }
});
```

#### Best Practices

1. **Input Validation**
   - Validate URLs and HTML content
   - Check prompt quality and clarity
   - Verify schema structure

```javascript
import { validateInput } from 'scrapegraph-js/utils';

const { isValid, errors } = validateInput({
  url: "https://example.com",
  html: "<div>content</div>",
  prompt: "Extract data",
  schema: { type: "object" }
});

if (!isValid) {
  console.error('Validation errors:', errors);
}
```

2. **HTML Content Handling**
   - Clean and normalize HTML
   - Handle encoding properly
   - Remove unnecessary scripts

```javascript
import { cleanHtml } from 'scrapegraph-js/utils';

const cleanedHtml = cleanHtml(rawHtml, {
  removeScripts: true,
  removeStyles: true,
  normalizeWhitespace: true
});

const request = getSmartScraperRequest({
  websiteHtml: cleanedHtml,
  userPrompt: "Extract the content"
});
```

3. **Error Recovery**
   - Implement retry logic
   - Handle rate limiting
   - Log errors for debugging

```javascript
import { getSmartScraperRequest, retry } from 'scrapegraph-js';

const scrapeWithRetry = retry(async (url, prompt) => {
  const request = await getSmartScraperRequest({
    websiteUrl: url,
    userPrompt: prompt
  });
  return request;
}, {
  retries: 3,
  backoff: {
    initial: 1000,
    multiplier: 2,
    maxDelay: 10000
  },
  onRetry: (error, attempt) => {
    console.log(`Retry attempt ${attempt} due to: ${error.message}`);
  }
});
```

4. **Performance Optimization**
   - Use caching strategies
   - Batch similar requests
   - Monitor resource usage

```javascript
import { cache, batch } from 'scrapegraph-js/utils';

// Cache requests
const cachedScraper = cache(getSmartScraperRequest, {
  ttl: 3600,
  maxSize: 100,
  keyGenerator: (params) => `${params.websiteUrl}-${params.userPrompt}`
});

// Batch requests
const batchScraper = batch(getSmartScraperRequest, {
  maxBatchSize: 10,
  batchDelay: 1000
});
```

### Custom Schema Example

Define exactly what data you want to extract:

<CodeGroup>

```python Python
from pydantic import BaseModel, Field

class ArticleData(BaseModel):
    title: str = Field(description="Article title")
    author: str = Field(description="Author name")
    content: str = Field(description="Main article content")
    publish_date: str = Field(description="Publication date")

response = client.smartscraper(
    website_url="https://example.com/article",
    user_prompt="Extract the article information",
    output_schema=ArticleData
)
```

```typescript TypeScript
import { z } from 'zod';

const ArticleSchema = z.object({
  title: z.string().describe('Article title'),
  author: z.string().describe('Author name'),
  content: z.string().describe('Main article content'),
  publishDate: z.string().describe('Publication date')
});

const response = await smartScraper(
  apiKey,
  'https://example.com/article',
  'Extract the article information',
  ArticleSchema
);
```

</CodeGroup>

### Async Support

For applications requiring asynchronous execution, SmartScraper provides async support through the `AsyncClient`:

```python
from scrapegraph_py import AsyncClient
import asyncio

async def main():
    async with AsyncClient(api_key="your-api-key") as client:
        response = await client.smartscraper(
            website_url="https://example.com",
            user_prompt="Extract the main content"
        )
        print(response)

# Run the async function
asyncio.run(main())
```

## Integration Options

### Official SDKs
- [Python SDK](/sdks/python) - Perfect for data science and backend applications
- [JavaScript SDK](/sdks/javascript) - Ideal for web applications and Node.js

### AI Framework Integrations
- [LangChain Integration](/integrations/langchain) - Use SmartScraper in your LLM workflows
- [LlamaIndex Integration](/integrations/llamaindex) - Build powerful search and QA systems

## Best Practices

### Optimizing Extraction
1. Be specific in your prompts
2. Use schemas for structured data
3. Handle pagination for multi-page content
4. Implement error handling and retries

### Rate Limiting
- Implement reasonable delays between requests
- Use async clients for better performance
- Monitor your [API usage](/dashboard/overview)

## Example Projects

Check out our [cookbook](/cookbook/introduction) for real-world examples:
- E-commerce product scraping
- News aggregation
- Research data collection
- Content monitoring

## API Reference

For detailed API documentation, see:
- [Start Scraping Job](/api-reference/endpoint/smartscraper/start)
- [Get Job Status](/api-reference/endpoint/smartscraper/get-status)

## Support & Resources

<CardGroup cols={2}>
  <Card title="Documentation" icon="book" href="/introduction">
    Comprehensive guides and tutorials
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/introduction">
    Detailed API documentation
  </Card>
  <Card title="Community" icon="discord" href="https://discord.gg/uJN7TYcpNa">
    Join our Discord community
  </Card>
  <Card title="GitHub" icon="github" href="https://github.com/ScrapeGraphAI">
    Check out our open-source projects
  </Card>
</CardGroup>

<Card title="Ready to Start?" icon="rocket" href="https://dashboard.scrapegraphai.com">
  Sign up now and get your API key to begin extracting data with SmartScraper!
</Card>
