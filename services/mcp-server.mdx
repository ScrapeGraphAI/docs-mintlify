---
title: 'üîå MCP Server'
description: 'Use ScrapeGraphAI through the Model Context Protocol (MCP)'
openapi: 'https://raw.githubusercontent.com/ScrapeGraphAI/scrapegraph-mcp/main/openapi.json'
---

# ScrapeGraph MCP Server

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)

A production‚Äëready Model Context Protocol (MCP) server that connects LLMs to the ScrapeGraph AI API for AI‚Äëpowered web scraping, research, and crawling.

<Card
  title="‚≠ê Star us on GitHub"
  icon="star"
  href="https://github.com/ScrapeGraphAI/scrapegraph-mcp"
>
  If this server is helpful, a star goes a long way. Thanks!
</Card>

## Key Features

- 8 tools covering markdown conversion, AI extraction, search, crawling, sitemap, and agentic flows
- Remote HTTP MCP endpoint and local Python server support
- Works with Cursor, Claude Desktop, and any MCP‚Äëcompatible client
- Robust error handling, timeouts, and production‚Äëtested reliability

## Get Your API Key

Create an account and copy your API key from the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com).

---

## Recommended: Use the Remote MCP Server

Endpoint:

```
https://mcp.scrapegraphai.com/mcp
```

Follow the instructions below:

### Cursor (HTTP MCP)

Add this to your Cursor MCP settings (`~/.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "scrapegraph-mcp": {
      "url": "https://mcp.scrapegraphai.com/mcp",
      "headers": {
        "X-API-Key": "YOUR_API_KEY"
      }
    }
  }
}
```


### Claude Desktop (via mcp-remote)

Claude Desktop connects to HTTP MCP via a lightweight proxy. Add the following to `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS (adjust path on Windows):

```json
{
  "mcpServers": {
    "scrapegraph-mcp": {
      "command": "npx",
      "args": [
        "mcp-remote@0.1.25",
        "https://mcp.scrapegraphai.com/mcp",
        "--header",
        "X-API-Key:YOUR_API_KEY"
      ]
    }
  }
}
```

### Smithery (optional)

```bash
npx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude
```

---

## Local Usage (Python)

Prefer running locally? Install and wire the server via stdio.

### Install

```bash
pip install -e .
# or
uv pip install -e .
```

Set your key:

```bash
# macOS/Linux
export SGAI_API_KEY=your-api-key-here
# Windows (PowerShell)
$env:SGAI_API_KEY="your-api-key-here"
```

### Run the server

```bash
scrapegraph-mcp
# or
python -m scrapegraph_mcp.server
```

<details open>
<summary><strong>Cursor (Local stdio)</strong></summary>

`~/.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "scrapegraph-mcp-local": {
      "command": "python",
      "args": ["-m", "scrapegraph_mcp.server"],
      "env": {
        "SGAI_API_KEY": "YOUR_API_KEY"
      }
    }
  }
}
```

</details>

<details open>
<summary><strong>Claude Desktop (Local stdio)</strong></summary>

`~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "scrapegraph-mcp-local": {
      "command": "python",
      "args": ["-m", "scrapegraph_mcp.server"],
      "env": {
        "SGAI_API_KEY": "YOUR_API_KEY"
      }
    }
  }
}
```

</details>

---

---

## Available Tools

The server exposes 8 enterprise‚Äëready tools:

### 1. markdownify
Convert a webpage to clean markdown.

```python
markdownify(website_url: str)
```

### 2. smartscraper
AI‚Äëpowered extraction with optional infinite scrolls.

```python
smartscraper(
  user_prompt: str,
  website_url: str,
  number_of_scrolls: int | None = None,
  render_heavy_js: bool | None = None
)
```

### 3. searchscraper
Search the web and extract structured results.

```python
searchscraper(
  user_prompt: str,
  num_results: int | None = None,
  number_of_scrolls: int | None = None
)
```

### 4. scrape
Fetch raw HTML with optional heavy JS rendering.

```python
scrape(website_url: str, render_heavy_js: bool | None = None)
```

### 5. sitemap
Discover a site‚Äôs URLs and structure.

```python
sitemap(website_url: str)
```

### 6. smartcrawler_initiate
Start an async multi‚Äëpage crawl (AI or markdown mode).

```python
smartcrawler_initiate(
  url: str,
  prompt: str | None = None,
  extraction_mode: str = "ai",
  depth: int | None = None,
  max_pages: int | None = None,
  same_domain_only: bool | None = None
)
```

### 7. smartcrawler_fetch_results
Poll results using the returned request_id.

```python
smartcrawler_fetch_results(request_id: str)
```

### 8. agentic_scrapper
Agentic, multi‚Äëstep workflows with optional schema and session persistence.

```python
agentic_scrapper(
  url: str,
  user_prompt: str | None = None,
  output_schema: dict | None = None,
  steps: list | None = None,
  ai_extraction: bool | None = None,
  persistent_session: bool | None = None,
  timeout_seconds: float | None = None
)
```

---

## Troubleshooting

- Verify your key is present in config (`X-API-Key` for remote, `SGAI_API_KEY` for local).
- Claude Desktop logs:
  - macOS: `~/Library/Logs/Claude/`
  - Windows: `%APPDATA%\\Claude\\Logs\\`
- If a long crawl is ‚Äústill running‚Äù, keep polling `smartcrawler_fetch_results`.

## License

MIT License ‚Äì see LICENSE file for details.


